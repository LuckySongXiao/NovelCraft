#!/usr/bin/env python3
"""
æµ‹è¯•é»˜è®¤AIé…ç½®è„šæœ¬
éªŒè¯Ollamaå’Œmollysama/rwkv-7-g1:0.4Bæ¨¡å‹çš„é…ç½®
"""

import asyncio
import httpx
import json
from typing import Dict, Any

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    "provider": "ollama",
    "base_url": "http://localhost:11434",
    "model": "mollysama/rwkv-7-g1:0.4B",
    "max_tokens": 2000,
    "temperature": 0.7
}

class OllamaTestClient:
    """Ollamaæµ‹è¯•å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        
    async def check_service(self) -> Dict[str, Any]:
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=10.0)
                response.raise_for_status()
                return {
                    "status": "success",
                    "message": "OllamaæœåŠ¡è¿è¡Œæ­£å¸¸",
                    "data": response.json()
                }
        except httpx.ConnectError:
            return {
                "status": "error",
                "message": "æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaå·²å®‰è£…å¹¶æ­£åœ¨è¿è¡Œ",
                "suggestion": "è¿è¡Œ 'ollama serve' å¯åŠ¨æœåŠ¡"
            }
        except httpx.TimeoutException:
            return {
                "status": "error",
                "message": "è¿æ¥OllamaæœåŠ¡è¶…æ—¶",
                "suggestion": "æ£€æŸ¥æœåŠ¡çŠ¶æ€æˆ–ç½‘ç»œè¿æ¥"
            }
        except Exception as e:
            return {
                "status": "error",
                "message": f"æ£€æŸ¥æœåŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "suggestion": "è¯·æ£€æŸ¥Ollamaå®‰è£…å’Œé…ç½®"
            }
    
    async def check_model(self, model_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.base_url}/api/tags", timeout=10.0)
                response.raise_for_status()
                models_data = response.json()
                
                available_models = [model["name"] for model in models_data.get("models", [])]
                
                if model_name in available_models:
                    return {
                        "status": "success",
                        "message": f"æ¨¡å‹ {model_name} å·²å®‰è£…å¹¶å¯ç”¨",
                        "available": True
                    }
                else:
                    return {
                        "status": "warning",
                        "message": f"æ¨¡å‹ {model_name} æœªå®‰è£…",
                        "available": False,
                        "suggestion": f"è¿è¡Œ 'ollama pull {model_name}' ä¸‹è½½æ¨¡å‹",
                        "available_models": available_models
                    }
        except Exception as e:
            return {
                "status": "error",
                "message": f"æ£€æŸ¥æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "available": False
            }
    
    async def test_generation(self, model_name: str, prompt: str = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚") -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹æ–‡æœ¬ç”Ÿæˆ"""
        try:
            data = {
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": DEFAULT_CONFIG["temperature"],
                    "num_predict": 100  # é™åˆ¶è¾“å‡ºé•¿åº¦ç”¨äºæµ‹è¯•
                }
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=data,
                    timeout=60.0
                )
                response.raise_for_status()
                result = response.json()
                
                return {
                    "status": "success",
                    "message": "æ¨¡å‹ç”Ÿæˆæµ‹è¯•æˆåŠŸ",
                    "prompt": prompt,
                    "response": result.get("response", ""),
                    "model": model_name
                }
        except Exception as e:
            return {
                "status": "error",
                "message": f"æ¨¡å‹ç”Ÿæˆæµ‹è¯•å¤±è´¥: {str(e)}",
                "suggestion": "è¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®å®‰è£…å’Œé…ç½®"
            }

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("NovelCraft é»˜è®¤AIé…ç½®æµ‹è¯•")
    print("=" * 60)
    print()
    
    # æ˜¾ç¤ºé»˜è®¤é…ç½®
    print("ğŸ“‹ é»˜è®¤é…ç½®ä¿¡æ¯:")
    for key, value in DEFAULT_CONFIG.items():
        print(f"   {key}: {value}")
    print()
    
    # åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
    client = OllamaTestClient(DEFAULT_CONFIG["base_url"])
    
    # 1. æ£€æŸ¥OllamaæœåŠ¡
    print("ğŸ” [1/3] æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€...")
    service_result = await client.check_service()
    print(f"   çŠ¶æ€: {service_result['status']}")
    print(f"   ä¿¡æ¯: {service_result['message']}")
    if service_result['status'] == 'error':
        print(f"   å»ºè®®: {service_result.get('suggestion', '')}")
        print("\nâŒ æµ‹è¯•ç»ˆæ­¢ï¼šOllamaæœåŠ¡ä¸å¯ç”¨")
        return
    
    if service_result['status'] == 'success':
        models_count = len(service_result['data'].get('models', []))
        print(f"   å·²å®‰è£…æ¨¡å‹æ•°é‡: {models_count}")
    print()
    
    # 2. æ£€æŸ¥é»˜è®¤æ¨¡å‹
    print("ğŸ” [2/3] æ£€æŸ¥é»˜è®¤æ¨¡å‹...")
    model_result = await client.check_model(DEFAULT_CONFIG["model"])
    print(f"   çŠ¶æ€: {model_result['status']}")
    print(f"   ä¿¡æ¯: {model_result['message']}")
    
    if not model_result['available']:
        print(f"   å»ºè®®: {model_result.get('suggestion', '')}")
        if model_result.get('available_models'):
            print("   å¯ç”¨æ¨¡å‹:")
            for model in model_result['available_models'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"     - {model}")
        print("\nâš ï¸  é»˜è®¤æ¨¡å‹æœªå®‰è£…ï¼Œè·³è¿‡ç”Ÿæˆæµ‹è¯•")
        return
    print()
    
    # 3. æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
    print("ğŸ” [3/3] æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    generation_result = await client.test_generation(DEFAULT_CONFIG["model"])
    print(f"   çŠ¶æ€: {generation_result['status']}")
    print(f"   ä¿¡æ¯: {generation_result['message']}")
    
    if generation_result['status'] == 'success':
        print(f"   æç¤º: {generation_result['prompt']}")
        print(f"   å›å¤: {generation_result['response'][:200]}...")  # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
    else:
        print(f"   å»ºè®®: {generation_result.get('suggestion', '')}")
    print()
    
    # æ€»ç»“
    print("=" * 60)
    if (service_result['status'] == 'success' and 
        model_result['available'] and 
        generation_result['status'] == 'success'):
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é»˜è®¤AIé…ç½®å·¥ä½œæ­£å¸¸ã€‚")
        print("ğŸ’¡ æ‚¨å¯ä»¥åœ¨NovelCraftä¸­æ­£å¸¸ä½¿ç”¨AIåŠŸèƒ½ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°å»ºè®®è¿›è¡Œé…ç½®ã€‚")
        print("ğŸ“– è¯¦ç»†é…ç½®æŒ‡å—è¯·å‚è€ƒ: Ollamaé»˜è®¤é…ç½®æŒ‡å—.md")
    print("=" * 60)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–åŒ…å®‰è£…")
